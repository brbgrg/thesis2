\section{GNN}

More recently, Hamilton et al. (2017) introduced GraphSAGE,
a method for computing node representations in an inductive manner. This technique operates by
sampling a fixed-size neighborhood of each node, and then performing a specific aggregator over
it (such as the mean over all the sampled neighbors’ feature vectors, or the result of feeding them
through a recurrent neural network). This approach has yielded impressive performance across several large-scale inductive benchmarks.
\cite{Velickovic2018}



Other
works preferred to learn the graph embedding using graph
attention network (GAT) which focuses on learning from
the most relevant nodes in the graph. For instance, [83]
proposed to combine the structural and functional brain
graphs into a single graph representation. Specifically, they
learn the embedding of a single graph that is represented by
an adjacency matrix denoting the structural connectivities
and a feature matrix denoting the functional connectivities.
The resulting learned latent representation was passed to
a multi-layer perceptron classifier to predict frontal lobe
epilepsy, temporal lobe epilepsy, and healthy subjects. All
the aforementioned models lack interpretation of the captured features in 
the embedding space which can help better
understand the original connectivity pattern that yields
the illness. Therefore, [84] another attention-based network,
proposed a combination of different GNN layers (Edge Weighted GAT layer, 
followed by Diffpool layers) to learn
the graph embedding and identify the Bipolar Disorder patients. Essentially, they performed a visual interpretation
of the attention map generated by the edge-weighted GAT
network so that they detect the particular associations of
abnormalities in the functional brain graph.
Challenges and insights. Building models that not only
predict the brain state of the subject but also explaining why
it makes such prediction, as well as integrating all types
of brain graphs in the learning process, is a compelling
research direction that remains unexplored.
\cite{Bessadok2022}


While effective when
compared to traditional machine learning methods, still
DL methods do not generalize well to non-Euclidean data
types (e.g., graphs). More specifically, directly applying DL
methods to graphs overlooks the relationships between
nodes and their local connectedness patterns. This causes an
important loss of topological properties inherently encoded
in a graph representation.
Recently, graph neural networks (GNNs) have been
proposed to tackle this issue. GNNs are the core of a
nascent field dealing with various graph-related tasks such
as graph classification and graph representation learning.
The main advantage of this domain is that it preserves graph topological properties 
while learning to perform a given task [42]–[44].
Such learning frameworks encapsulate both
graph representation learning via embedding and scoring
prediction via different operations such as pooling.

a set of ML methods have been developed for
learning the embedding of brain graphs into different
spaces such as geometric and hyperbolic spaces, thereby
enabling a better visualization of the complex topology of
brain connectivity. 
For instance, x proposed ...

While several review papers already exist,
they are all different from our review. For instance, [42]–[44],
[46], [47] and [1]–[6], [14], [16], [48] do not discuss specific
GNN-based methods for solving neuroscience problems,
but instead act as a reference for a specific topic (i.e., GNN or
neuroscience)
\cite{Bessadok2022}


Deep learning is a promising data-driven tool to automatically learn complex
490 feature representations in large data. Several deep learning approaches exist to
491 understand the human brain network [13,35,48]. A variety of deep methods have
492 been applied to fMRI connectome data, such as the feedforward neural net
493 works (FNN) [33], long short-term memory (LSTM) recurrent neural networks
494 [9], and 2D convolutional neural networks (CNN) [24]. However, these existing
495 deep learning methods for fMRI analysis usually need to learn around millions
496 of parameters due to the high dimensionality of fMRI connectome, thus larger
497 datasets are required to train the models. Compared with the above mentioned
498 deep learning methods, GNNs require much fewer parameters and are ideal for
499 graph-structured data analysis. Hammond et al. [20] proposed a spectral graph
500 convolution method which defines convolution for graphs in the spectral do
501 main. Later, Defferrard et al. [10] simplified spectral graph convolution to a
502 local form and Kipf et al. introduced the Graph Convolutional Neural Network
503 (GCN) [27] which provided an approximated fast computation. Hamilton et al.
504 [19] proposed another variant of graph convolution in the spatial domain that
\cite{Li2020}

Relying on its non-Euclidean data type, graph neural network (GNN)
provides a clever way of learning the deep graph structure and it is 
rapidly becoming the state-of-the-art leading to enhanced
performance in various network neuroscience tasks. Here we review current 
GNN-based methods, highlighting the ways that they have
been used in several applications related to brain graphs such as missing 
brain graph synthesis and disease classification. We
conclude by charting a path toward a better application of GNN models in 
network neuroscience field for neurological disorder
diagnosis and population graph integration.
\cite{Bessadok2022}


\section{Deep brain network representation learning}
With the development of artificial intelligence (AI) techniques,
learning-based methods (e.g., machine learning, deep learning) are
broadly investigated and applied to brain network data for different
research purposes. Most of these learning methods are based on the graph
neural networks (GNN), a class of deep neural networks for graph structured 
data representations.27–29,124–128 Many research objectives
on brain network learning have been proposed in recent years. For
example, a few studies focus on developing deep learning methods to
model the multiview representations across different modalities-derived
brain network data.
\cite{Tang2023}